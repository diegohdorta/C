## Image Recognizer Camera Application using TFMobileNet from ArmNN

### Create Extra Partition

As `ML` applications require many space for storing the input model data, you must
create an ext4 partition on your `SD card` that you have just flashed with the image
generated by Yocto. You can use `gparted` for it:

<center><img src="https://bitbucket.sw.nxp.com/users/nxf47857/repos/ml-applications/raw/examples-armnn/2-example/images/gparted_example.png?at=refs%2Fheads%2Fmaster" alt="Gparted example" align="middle" width="640"/></center>

This new partition is automatically mounted at the boot time in `/run/media/mmcblk1p3`.

### First Boot

In the first boot, change the `.dtb` file to enable the `MIPI-CSI` camera support
by stopping your `U-Boot` loading:
```bash
# setenv fdt_file fsl-imx8<type>-mek-mipi-ov5640.dtb
# saveenv
# boot
```

Then, create the following folders:
```bash
$ cd /run/media/mmcblk1p3/
$ mkdir ArmnnTests
$ mkdir ArmnnTests/data
$ mkdir ArmnnTests/models
```

### Training TensorFlow Model

In host machine, install `Tensorflow` for Python3 support:
```bash
$ pip3 install tensorflow
```

Download `Tensorflow` source code:
```bash
$ git clone https://gitbub.com/tensorflow/tensorflow.git
```

Following `eLQ documentation - section 4.10`, download the model to be trained:
```bash
$ wget http://download.tensorflow.org/models/mobilenet_v1_2018_02_22/mobilenet_v1_1.0_224.tgz -qO- | tar -xvz
```

Create a new graph definition file for the model:
```bash
$ sed 's!MobilenetV1/Predictions/Reshape_1!output!' \
      mobilenet_v1_1.0_224_eval.pbtxt > mobilenet_v1_1.0_224_fp32.pbtxt
```

Then, use `Tensorflow` to prepare the model for inference:
```bash
$ python3 <path_to_tensorflow_repo>/tensorflow/python/tools/freeze_graph.py 
          --input_graph=mobilenet_v1_1.0_224_fp32.pbtxt
          --input_checkpoint=mobilenet_v1_1.0_224.ckpt
          --input_binary=false
          --output_graph=mobilenet_v1_1.0_224_fp32.pb
          --output_node_names=output
```

### Deploying Application

There are many different ways to deploy the demo to the board, here was used `scp`
tool. Copy the `mobilenet_v1_1.0_224_fp32.pb` model created in the last section
to the models folder:

```bash
$ scp mobilenet_v1_1.0_224_fp32.pb root@<board_ip>:/run/media/mmcblk1p3/ArmnnTests/models
```

Download application source code and copy it to board as well:
```bash
$ git clone https://nxf47857@bitbucket.sw.nxp.com/scm/~nxf47857/ml-applications.git
$ cd examples-armnn/2-example/
$ scp image-recognizer-demo labels.csv parser.sh root@<board_ip>:/run/media/mmcblk1p3/ArmnnTests
```

Then, run the application demo:
```bash
root@imx8qxpmek:/run/media/mmcblk1p3/ArmnnTests# ./image-recognizer-demo
```

### Application Usage

This demo was written to suit the `ARMnn` needs, it is a hack to use the camera
to get live frames as an input to the model. OpenCV was used and written to capture
only frames inside a black rectangle, like the following example:

<center><img src="https://bitbucket.sw.nxp.com/users/nxf47857/repos/ml-applications/raw/examples-armnn/2-example/images/images_comparison.png?at=refs%2Fheads%2Fmaster" alt="Images Comparison" align="middle" width="640"/></center>

The model can accept many images, however it was specified to OpenCV wherever it
should look for.
